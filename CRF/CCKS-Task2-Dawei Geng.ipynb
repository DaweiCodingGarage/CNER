{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运用条件随机场(CRF)进行医疗数据的实体识别 -- CCKS Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = ['检查和检验', '治疗', '疾病和诊断', '症状和体征', '身体部位']\n",
    "label2id = {l:ind for ind, l in enumerate(label)}\n",
    "id2label = {ind:l for ind, l in enumerate(label)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import csv\n",
    "\n",
    "#jieba.load_userdict(\"字典w词性.txt\")\n",
    "\n",
    "\n",
    "label_set = {}\n",
    "label_data_list = []\n",
    "entity_stats = False\n",
    "\n",
    "feature_dict = defaultdict(dict)\n",
    "directory = os.path.join(\"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\",\"task2data\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for ind, file in enumerate(files):\n",
    "        if file.endswith(\".txt\"):\n",
    "            if ind % 2 == 0:\n",
    "                #tree = Trie()\n",
    "                with open(root+'\\\\'+file,'r',encoding='utf-8') as infile:\n",
    "                    reader = csv.reader(infile,delimiter='\\t')\n",
    "                    tag_seq = []\n",
    "                    word_set = set()\n",
    "                    label_dict ={0:[], 1:[], 2:[], 3:[], 4:[]}\n",
    "                    \n",
    "                    for line in reader:\n",
    "                        if line:\n",
    "                            #记录Entity的位置\n",
    "                            tag_seq.append((line[1],line[2],label2id[line[3]]))\n",
    "                            label_dict[label2id[line[3]]].append(line[0])   \n",
    "                            ##统计Entity\n",
    "                            if entity_stats:\n",
    "                                \n",
    "                                #统计每类Entity的个数    \n",
    "                                if line[-1] in label_set:\n",
    "                                    label_set[line[-1]] += 1\n",
    "                                else:\n",
    "                                    label_set[line[-1]] = 1\n",
    "                                \n",
    "                            if line[0] not in word_set:\n",
    "                                word_set.add(line[0])\n",
    "                                #tree.insert(list(pseg.cut(line[0])),label2id[line[-1]])\n",
    "                            \n",
    "                    \n",
    "                #label_df = pd.read_csv(root+'\\\\'+file,encoding='utf-8', sep='\\t',names=['实体','start_pos','end_pos','label'])\n",
    "            else:\n",
    "                #print(file)\n",
    "                data_label = []\n",
    "                with open(root+'\\\\'+file,'r',encoding='utf-8') as infile:\n",
    "                    reader = csv.reader(infile,delimiter='\\n')\n",
    "                    for line in reader:\n",
    "                        if line:\n",
    "                            temp = line[0].replace(\"\\t\",\" \")\n",
    "                            data_label += list(temp)\n",
    "                        \n",
    "                #print(question)\n",
    "                data_label = [[i,j] for i, j in zip(data_label,['O',]*len(data_label))]\n",
    "                #print(len(data_label),len(tag_seq))\n",
    "                if len(tag_seq):\n",
    "                    for start,end,tag in tag_seq:\n",
    "                        for i in range(int(start),int(end)+1):\n",
    "                            if i == int(start):\n",
    "\n",
    "                                data_label[i][-1] ='B-'+str(tag)\n",
    "                            else:\n",
    "                                data_label[i][-1] ='I-'+str(tag)\n",
    "                \n",
    "                            \n",
    "                label_data_list.append((data_label,label_dict))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'检查和检验': 5785, '治疗': 712, '疾病和诊断': 604, '症状和体征': 6187, '身体部位': 8310}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载外部字典，总结字典n-gram的规律 (字典来源： ICD9, ICD10等 合计共7万医学名词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_med_set(dict_name):\n",
    "    med_set = set()\n",
    "    med_dict = pd.read_csv(dict_name,names=['word'])\n",
    "    for word in med_dict['word'].tolist():\n",
    "        if word not in med_set:\n",
    "            med_set.add(word)\n",
    "    return med_set\n",
    "\n",
    "med_set = load_med_set('字典全.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结常见的医学前缀和后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_suffix(med_set, cutoff):\n",
    "    suffix_dict = {}\n",
    "    for i in med_set:\n",
    "        if len(i)>=4:\n",
    "            word_list = [i[-2:],i[-3:],i[-4:]]\n",
    "        elif len(i)>=3:\n",
    "            word_list = [i[-2:],i[-3:]]\n",
    "        elif len(i)>=2:\n",
    "            word_list = [i[-2:]]\n",
    "        else:\n",
    "            word_list=[]\n",
    "        \n",
    "        for word in word_list:\n",
    "            if word not in suffix_dict:\n",
    "                suffix_dict[word] = 1\n",
    "            else:\n",
    "                suffix_dict[word] += 1\n",
    "    return {key:value for key,value in suffix_dict.items() if value>=cutoff}\n",
    "\n",
    "def common_prefix(med_set,cutoff):\n",
    "    prefix_dict = {}\n",
    "    for i in med_set:\n",
    "        if len(i)>=4:\n",
    "            word_list = [i[:2],i[:3],i[:4]]\n",
    "        elif len(i)>=3:\n",
    "            word_list = [i[:2],i[:3]]\n",
    "        elif len(i)>=2:\n",
    "            word_list = [i[:2]]\n",
    "        else:\n",
    "            word_list=[]\n",
    "        \n",
    "        for word in word_list:\n",
    "            if word not in prefix_dict:\n",
    "                prefix_dict[word] = 1\n",
    "            else:\n",
    "                prefix_dict[word] += 1\n",
    "    return {key:value for key,value in prefix_dict.items() if value>=cutoff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72517 31385\n"
     ]
    }
   ],
   "source": [
    "def add_word_from_dict(dict_name,label,current_set,current_dict):\n",
    "    med_dict = pd.read_csv('./dictionary/CLEAN_DICT/'+dict_name,names=['word'])\n",
    "    for word in med_dict['word'].tolist():\n",
    "        word = word.strip(\"\\u3000\\u3000\")\n",
    "        if word not in current_set:\n",
    "            current_set.add(word)\n",
    "        if word not in current_dict:\n",
    "            current_dict[word] = label\n",
    "    return current_set, current_dict\n",
    "\n",
    "curr_set, curr_dict = add_word_from_dict('icd_9.csv',1,med_set,{})\n",
    "curr_set, curr_dict = add_word_from_dict('ICD10.csv',2,curr_set, curr_dict)\n",
    "curr_set, curr_dict = add_word_from_dict('med_online.csv',1,curr_set, curr_dict)\n",
    "curr_set, curr_dict = add_word_from_dict('medicine.csv',1,curr_set, curr_dict)\n",
    "curr_set, curr_dict = add_word_from_dict('pathology.csv',2,curr_set, curr_dict)\n",
    "label_set_all,label_dict_all = add_word_from_dict('中文身体部位名称.txt',4,curr_set, curr_dict)\n",
    "print(len(label_set_all),len(label_dict_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77658 92422\n"
     ]
    }
   ],
   "source": [
    "word_set_suffix =set()\n",
    "word_set_prefix = set()\n",
    "\n",
    "suffix = common_suffix(med_set,1)\n",
    "prefix = common_prefix(med_set,1)\n",
    "for key, value in suffix.items():    \n",
    "    word_set_suffix.add(key)\n",
    "\n",
    "for key, value in prefix.items():    \n",
    "    word_set_prefix.add(key)\n",
    "        \n",
    "    \n",
    "print(len(word_set_suffix),len(word_set_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  根据外部字典所有可能出现在医学里的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123\n"
     ]
    }
   ],
   "source": [
    "def all_possible_char(word_set):\n",
    "    all_char_set = set()\n",
    "    for word in word_set:\n",
    "        for char in list(word):\n",
    "            all_char_set.add(char)\n",
    "    return all_char_set\n",
    "dict_set = all_possible_char(label_set_all)\n",
    "print(len(dict_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结以每个字为中心，左右两个字的窗口内的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def word2features_1(sent, i):\n",
    "    word = sent[i][0]\n",
    "    #postag = sent[i][1]\n",
    "    #head = sent[i][2]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        #'head': head,\n",
    "        'word':word,\n",
    "        'word.ispunc()': 1 if re.match('^[a-zA-Z0-9_]*$',word) else 0,\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isalpha()': word.isalpha(),\n",
    "        #'word.prefix': word in word_set_prefix,\n",
    "        #'word.suffix':word in word_set_suffix,\n",
    "        #'word.dict': word in dict_set,\n",
    "        #'postag': postag,\n",
    "        #'postag_n': postag[0]=='n' and postag!='ng',\n",
    "        #'head_postag':str(head)+\"|\"+postag,\n",
    "        \n",
    "       # 'postag[:1]': postag[:1]\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        #postag1 = sent[i-1][1]\n",
    "        #head1 = sent[i-1][2]\n",
    "        features.update({\n",
    "           #'-1:head': head1,\n",
    "            '-1:word':word1,\n",
    "            #'-1:word.dict': word1 in dict_set,\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:word.isalpha()': word1.isalpha(),\n",
    "            '-1:word.ispunc()': 1 if re.match('^[a-zA-Z0-9_]*$',word1) else 0,\n",
    "            #'-1:word.prefix': word1 in word_set_prefix,\n",
    "            #'-1:word.suffix':word1 in word_set_suffix,\n",
    "            '-1:0.word.suffix':   word1+word in word_set_suffix,\n",
    "            '-1:0.word.prefix':   word1+word in word_set_prefix,   \n",
    "           # '-1:postag': postag1,\n",
    "            \n",
    "            \n",
    "            '-1:0': word1+word,\n",
    "            #'-1:0_pos': postag1+'|'+postag,\n",
    "           # '-1:postag_n': postag1[0]=='n' and postag1!='ng',\n",
    "           \n",
    "             \n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word2 = sent[i+1][0]\n",
    "        #postag2 = sent[i+1][1]\n",
    "        #head2 = sent[i+1][2]\n",
    "        features.update({\n",
    "           #'+1:head': head2, \n",
    "            '+1:word':word2,\n",
    "            #'+1:word.dict': word2 in dict_set,\n",
    "            '+1:word.isdigit()': word2.isdigit(),\n",
    "            '+1:word.isalpha()': word2.isalpha(),\n",
    "            '+1:word.ispunc()': 1 if re.match('^[a-zA-Z0-9_]*$',word2) else 0,\n",
    "            #'+1:postag': postag2,\n",
    "            #'+1:word.prefix': word2 in word_set_prefix,\n",
    "            #'+1:word.suffix':word2 in word_set_suffix,\n",
    "            '+1:0': word+word2,\n",
    "            '+1:0.word.suffix':   word+word2 in word_set_suffix,\n",
    "            '+1:0.word.prefix':   word+word2 in word_set_prefix,\n",
    "            #'+1:0_pos': postag+'|'+postag2,\n",
    "            #'+1:postag_n': postag2[0]=='n' and postag2!='ng',\n",
    "              \n",
    "           \n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "        \n",
    "   \n",
    "        \n",
    "    if i > 1:\n",
    "        word3 = sent[i-2][0]\n",
    "        #postag3 = sent[i-2][1]\n",
    "        #head3 = sent[i-2][2]\n",
    "        features.update({\n",
    "            #'-2:head': head3,\n",
    "            '-2:word': word3,\n",
    "           # '-2:word.prefix': word3 in word_set_prefix,\n",
    "           # '-2:word.suffix':word3 in word_set_suffix,\n",
    "            '-2:-1:word': word3+word1,\n",
    "             #'-2:-1:postag': postag3+'|'+postag1,\n",
    "            '-2:-1:0_word': word3+word1+word,\n",
    "            #'-2:-1:0_pos': postag3+'|'+postag1+'|'+postag\n",
    "           '-2:-1.word.suffix':  word3+word1 in word_set_suffix,\n",
    "            '-2:-1.word.prefix':   word3+word1 in word_set_prefix,\n",
    "            '-2:-1:0.word.suffix':  word3+word1+word in word_set_suffix,\n",
    "            '-2:-1:0.word.prefix':   word3+word1+word in word_set_prefix,\n",
    "           \n",
    "        })\n",
    "        \n",
    "    if i < len(sent)-2:\n",
    "        word4 = sent[i+2][0]\n",
    "        #postag4 = sent[i+2][1]\n",
    "        #head4 = sent[i+2][2]\n",
    "        features.update({\n",
    "            #'+2:head': head4,\n",
    "                '+2:word': word4,\n",
    "            #'+2:word.prefix': word4 in word_set_prefix,\n",
    "           # '+2:word.suffix':word4 in word_set_suffix,\n",
    "             \n",
    "            '+2:+1:word': word2+word4,\n",
    "            '+2:+1.word.suffix': word2+word4 in word_set_suffix,\n",
    "            '+2:+1.word.prefix':   word2+word4 in word_set_prefix,\n",
    "            #'+2:+1:postag': postag2+'|'+postag4,\n",
    "            '+2:+1:0_word': word+word2+word4,\n",
    "            '+2:+1:0.word.suffix':  word+word2+word4 in word_set_suffix,\n",
    "            '+2:+1:0.word.prefix':   word+word2+word4 in word_set_prefix,\n",
    "            #'+2:+1:0_pos': postag+'|'+postag2+'|'+postag4\n",
    "              \n",
    "           \n",
    "        })\n",
    "   \n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features_1(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出数据，评估数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_data_transform(docs):\n",
    "    input_data = []\n",
    "    for i in map(lambda x:list(zip(*x)),docs):\n",
    "        input_data.append([list(j) for j in i])\n",
    "\n",
    "    return input_data\n",
    "\n",
    "def write_conll(fstream, data):\n",
    "    \"\"\"\n",
    "    Writes to an output stream @fstream (e.g. output of `open(fname, 'r')`) in CoNLL file format.\n",
    "    @data a list of examples [(tokens), (labels), (predictions)]. @tokens, @labels, @predictions are lists of string.\n",
    "    \"\"\"\n",
    "    for cols in data:\n",
    "        for row in zip(*cols):\n",
    "            fstream.write(\"\\t\".join([str(i) for i in row]))\n",
    "            fstream.write(\"\\n\")\n",
    "        fstream.write(\"\\n\")\n",
    "\n",
    "def test_ner(index):\n",
    "    script_file = \"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\\\\Assignment3\\\\assignment3\\\\conlleval\"\n",
    "    output_file = \"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\\\\Assignment3\\\\assignment3\\\\final\\\\pred{}.conll\".format(index)\n",
    "    result_file = \"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\\\\Assignment3\\\\assignment3\\\\final\\\\ner_result{}.utf8\".format(index)\n",
    "    \n",
    "    os.system(\"perl {} < {} > {}\".format(script_file, output_file, result_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据转化为可提交的格式的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "def get_entity_type(tag_list):\n",
    "    normal_tag_list = ['0','1','2','3','4']\n",
    "    max_count = 0\n",
    "    for tag in normal_tag_list:\n",
    "        if tag_list.count(tag) > max_count:\n",
    "            max_count = tag_list.count(tag)\n",
    "            real_tag = tag\n",
    "    return real_tag\n",
    "\n",
    "def generate_result(file_path):\n",
    "    sents=[]  #用于保存结果的列表\n",
    "    tempLine=[] #用于保存当前句的临时列表\n",
    "    for eachLine in open(file_path,'r',encoding='utf8'): #逐行读取标结果文件\n",
    "        if(eachLine!='\\n'): #如果当前行不为空\n",
    "            colList=eachLine.strip('\\n').split('\\t') #根据制表符进行分隔，得到当前行的各列数据\n",
    "            #print(colList)\n",
    "            tempLine.append([colList[0],colList[1]]) # 将其追加到临时列表中\n",
    "        else: #如果当前为空行\n",
    "            sents.append(tempLine[:]) #说明一句读完了，则添加到结果列表中\n",
    "            tempLine=[] #清空临时列表，等待下一句\n",
    "    #print(sents)\n",
    "    #print(len(sents))\n",
    "\n",
    "    final_results=[] #用于保存最终结果的列表\n",
    "    for sentId in range(len(sents)): #遍历上述代码片断的结果中的句子\n",
    "        sentence_result = []\n",
    "        entity_word='' #用于保存当前捕捉到的结果的临时列表\n",
    "        tag_list = []\n",
    "        firstWordId=0 #第一个遍历当前句子的游标\n",
    "        while(firstWordId<len(sents[sentId])-1): # 开始循环\n",
    "            if(sents[sentId][firstWordId][-1]!='O' ) and ('B-' in sents[sentId][firstWordId][-1]): #如果发现有非O行\n",
    "                secondWordId=firstWordId+1 #设置第二个遍历当前句子的游标，从第一个游标的下一元素开始\n",
    "                tag_list.append(sents[sentId][firstWordId][-1].split('-')[-1])\n",
    "                entity_word += sents[sentId][firstWordId][0] #将当前行添加到临时表中\n",
    "                start_index = firstWordId\n",
    "                while(secondWordId<len(sents[sentId])): #开始第二个游标的循环\n",
    "                    if(sents[sentId][secondWordId][-1]!='O') and ('B-' not in sents[sentId][secondWordId][-1]): #如果发现非O行\n",
    "                    #if(sents[sentId][secondWordId][-1]!='O'): #如果发现非O行\n",
    "                        entity_word += sents[sentId][secondWordId][0] #将当前行添加到临时表中\n",
    "                        tag_list.append(sents[sentId][secondWordId][-1].split('-')[-1])\n",
    "                    else: #如果当前行的标注结果是O，说明前面发现的标结果已经捕捉完毕，\n",
    "                        break #中断第二个游标的循环\n",
    "                    secondWordId+=1\n",
    "                    firstWordId+=1\n",
    "                real_tag = get_entity_type(tag_list)\n",
    "                index = (start_index,secondWordId)\n",
    "                sentence_result.append({'index':index, 'value':(real_tag, entity_word)})\n",
    "                #当内层循环结束时，说明已经发现了一组标注结果了，将他们整体添加到最终结果列表中\n",
    "                entity_word='' #清空临时列表\n",
    "                tag_list = []\n",
    "            firstWordId+=1 #改变外层循环变量\n",
    "        final_results.append(sentence_result)\n",
    "        #print(final_results)\n",
    "    return final_results\n",
    "\n",
    "\n",
    "\n",
    "def get_str_result(file_path):\n",
    "    tag_mapping_dict = {'0':u'检查和检验','1':u'治疗', '2':u'疾病和诊断', '3':u'症状和体征', '4':u'身体部位'}\n",
    "    final_result = generate_result(file_path)\n",
    "    #final_result = post_process(final_result)\n",
    "    final_str_result =[]\n",
    "    for i in range(len(final_result)):\n",
    "        all_text_str = ''\n",
    "        all_text_list = []\n",
    "        for dict_ in final_result[i]:\n",
    "            text_str = ' '.join([dict_['value'][1],str(dict_['index'][0]),\\\n",
    "                                 str(dict_['index'][1]-1),tag_mapping_dict[dict_['value'][0]]])\n",
    "            all_text_list.append(text_str[:])\n",
    "        all_text_str = ';'.join(all_text_list)\n",
    "        final_str_result.append(all_text_str)\n",
    "    return final_str_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,csv\n",
    "data_list = []\n",
    "file_list = []\n",
    "directory = os.path.join(\"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\",\"task2test\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for ind, file in enumerate(files):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_cat, file_ind = file.split(\".\")[0].split(\"-\")\n",
    "            file_list.append(\",\".join([file_ind,file_cat]))\n",
    "                #print(file)\n",
    "            data_label = []\n",
    "            with open(root+'\\\\'+file,'r',encoding='utf-8') as infile:\n",
    "                reader = csv.reader(infile,delimiter='\\n')\n",
    "                for line in reader:\n",
    "                    if line:\n",
    "                        temp = line[0].replace(\"\\t\",\" \")\n",
    "                        data_label += list(temp)\n",
    "\n",
    "            \n",
    "            data_list.append(data_label[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找合适的hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 77.2min\n",
      "[Parallel(n_jobs=3)]: Done 150 out of 150 | elapsed: 421.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=None,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error...e,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False),\n",
       "          fit_params={}, iid=True, n_iter=50, n_jobs=3,\n",
       "          param_distributions={'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000002D903BDA20>, 'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000002DEE22C940>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring=make_scorer(flat_f1_score, labels=['B-0', 'I-0', 'B-1', 'I-1', 'B-2', 'I-2', 'B-3', 'I-3', 'B-4', 'I-4'], average=weighted),\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "result = [label_data_list[i][0] for i in range(len(label_data_list))]\n",
    "\n",
    "X = [sent2features(s) for s in result]\n",
    "y = [sent2labels(s) for s in result]\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted',labels=['B-0','I-0','B-1','I-1','B-2','I-2','B-3','I-3','B-4','I-4'])\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=3,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c1': 0.088599253829169486, 'c2': 0.0046537586495652107}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证评估模型表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92994874237062708]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "f1_score = []\n",
    "result = [label_data_list[i][0] for i in range(len(label_data_list))]\n",
    "\n",
    "X = [sent2features(s) for s in result]\n",
    "y = [sent2labels(s) for s in result]\n",
    "\n",
    "kf = KFold(len(result), n_folds=10,shuffle=True,random_state=1)\n",
    "\n",
    "index = 0\n",
    "for train_index, test_index in kf:\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1= 0.089,\n",
    "        c2= 0.004,\n",
    "        max_iterations=500,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = crf.predict(X_test)\n",
    "    \n",
    "    f1 = metrics.flat_f1_score(y_test, y_pred,\n",
    "                          average='weighted', labels=['B-0','I-0', 'B-1','I-1','B-2','I-2','B-3','I-3','B-4','I-4'])\n",
    "    \n",
    "        \n",
    "    f1_score.append(f1)\n",
    "    \n",
    "    test_char = [result[i] for i in test_index]\n",
    "    datawpred = [[[data[0],data[-1]]+[pred] for data, pred in zip(test_char[j],y_pred[j])] for j in range(len(y_pred))]\n",
    "    with open(\"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\\\\Assignment3\\\\assignment3\\\\final\\\\pred{}.conll\".format(index),\\\n",
    "              'w',encoding='utf-8') as f:\n",
    "        write_conll(f, input_data_transform(datawpred))\n",
    "    test_ner(index)\n",
    "\n",
    "    index += 1\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到测试数据的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.cross_validation import KFold\n",
    "#f1_score = []\n",
    "result = [i[0] for i in label_data_list]\n",
    "X = [sent2features(s) for s in result]\n",
    "y = [sent2labels(s) for s in result]\n",
    "X_test = [sent2features(s) for s in data_list]\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X, y)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "test_char = X_test\n",
    "datawpred = [[[data,pred] for data, pred in zip(data_list[j],y_pred[j])] for j in range(len(y_pred))]\n",
    "with open(\"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\\\\Assignment3\\\\assignment3\\\\final\\\\pred{}.conll\".format(11),\\\n",
    "          'w',encoding='utf-8') as f:\n",
    "    write_conll(f, input_data_transform(datawpred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "final = get_str_result(\"C:\\\\Users\\\\Dawei\\\\Downloads\\\\NER\\\\Assignment3\\\\assignment3\\\\final\\\\pred{}.conll\".format(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('result.csv','w',encoding='utf-8') as f:\n",
    "   \n",
    "    for i in range(len(final)):\n",
    "        if final[i]:\n",
    "            f.write(file_list[i]+\",\"+final[i]+\";\")\n",
    "            f.write(\"\\n\")\n",
    "        else:\n",
    "            f.write(file_list[i]+\",\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 由CRF得到的状态转移矩阵以及特征效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "I-1    -> I-1     4.968283\n",
      "I-2    -> I-2     4.243605\n",
      "I-4    -> I-4     4.080847\n",
      "O      -> B-1     4.026180\n",
      "B-3    -> I-3     3.795849\n",
      "I-0    -> I-0     3.646458\n",
      "O      -> O       3.488403\n",
      "B-0    -> I-0     3.073345\n",
      "I-3    -> I-3     3.047348\n",
      "B-2    -> I-2     2.955577\n",
      "\n",
      "Top unlikely transitions:\n",
      "O      -> I-4     -14.387559\n",
      "O      -> I-0     -12.631001\n",
      "O      -> I-1     -11.502036\n",
      "O      -> I-2     -11.324096\n",
      "I-4    -> I-0     -10.761696\n",
      "I-4    -> I-2     -10.099810\n",
      "B-4    -> I-0     -9.719350\n",
      "B-4    -> I-2     -9.673938\n",
      "O      -> I-3     -9.505175\n",
      "I-4    -> I-1     -8.633114\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-10:][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "10.806019 O        word:，\n",
      "10.283376 O        word:。\n",
      "10.094325 O        word:：\n",
      "9.248702 B-0      +1:0:压痛\n",
      "9.208959 O        word::\n",
      "8.920277 O        word:、\n",
      "8.413235 O        word:及\n",
      "8.241644 O        word:；\n",
      "8.130099 O        word:\"\n",
      "7.501996 I-4      -1:word:脊\n",
      "7.496708 O        word:与\n",
      "7.482643 O        word:未\n",
      "7.170733 I-0      -1:0:查体\n",
      "6.710941 O        +2:+1:0_word:木，结\n",
      "6.590223 O        word:1\n",
      "6.456654 O        word:无\n",
      "6.436837 O        word:伴\n",
      "6.434000 B-4      +2:+1:0_word:痰，左\n",
      "6.314114 I-4      word:趾\n",
      "6.195015 B-4      word:肺\n",
      "6.180917 O        +2:+1:0_word:吐。4\n",
      "6.179230 I-0      word:片\n",
      "6.177268 I-2      word:炎\n",
      "6.155166 I-4      word:区\n",
      "6.129004 I-4      word:道\n",
      "6.113960 I-3      -1:0:无力\n",
      "6.079527 B-4      +1:0:腹软\n",
      "6.069166 I-4      word:膜\n",
      "6.062591 B-4      -2:-1:0_word:肿，心\n",
      "6.027395 I-4      word:便\n",
      "6.004802 B-3      +1:0:渗出\n",
      "5.930228 I-0      -1:0:压痛\n",
      "5.888827 O        word:可\n",
      "5.841146 B-4      -2:-1:0_word:可，肌\n",
      "5.734185 I-0      word:音\n",
      "5.666206 I-4      -1:0:口唇\n",
      "5.631642 B-0      -1:word:查\n",
      "5.614429 O        word:2\n",
      "5.569294 I-4      -1:word:右\n",
      "5.560337 O        word:以\n",
      "5.536121 I-4      -1:word:左\n",
      "5.532155 O        -2:-1:0_word:2、高\n",
      "5.500975 I-3      -1:0:畸形\n",
      "5.500975 I-3      -1:word:畸\n",
      "5.458605 I-4      word:管\n",
      "5.427430 I-4      -1:0:颅内\n",
      "5.421674 I-3      -1:0:黄染\n",
      "5.396356 O        -1:word:片\n",
      "5.365134 I-0      +2:+1:0_word:物。拔\n",
      "5.363395 B-3      word:痛\n",
      "5.349087 B-3      word:肿\n",
      "5.337837 B-3      +2:+1:0_word:胀痛，\n",
      "5.335513 B-0      word:P\n",
      "5.326042 B-4      -1:word:痛\n",
      "5.310861 B-3      +1:0:气短\n",
      "5.295723 O        -2:-1:0_word:，诉腰\n",
      "5.287367 B-4      +1:0:肛门\n",
      "5.279051 O        word:有\n",
      "5.276521 I-4      word:节\n",
      "5.273265 O        -1:word:术\n",
      "5.214701 I-4      word:门\n",
      "5.198499 O        word:>\n",
      "5.170842 B-3      +1:0:结节\n",
      "5.164783 B-4      +2:+1:0_word:涕，饮\n",
      "5.162672 O        +2:+1:0_word:比7.\n",
      "5.157050 I-4      word:部\n",
      "5.141769 O        -2:-1:0_word:生，膀\n",
      "5.132573 O        word:为\n",
      "5.092382 B-4      +1:0:脾未\n",
      "5.079153 O        +2:+1:0_word:肿，心\n",
      "5.039232 B-0      +1:0:查体\n",
      "4.980645 I-0      -1:0:细胞\n",
      "4.945238 I-2      word:癌\n",
      "4.934521 B-0      +1:0:触痛\n",
      "4.931142 O        +2:+1:0_word:胀，胸\n",
      "4.923878 O        -2:-1:0_word:l，注\n",
      "4.903727 B-4      -2:-1:0_word:）：颅\n",
      "4.902697 I-3      -1:0:充血\n",
      "4.881833 I-4      -2:-1:0_word:无心包\n",
      "4.860340 O        -2:-1:0_word:音，双\n",
      "4.850966 B-3      +1:0:血肿\n",
      "4.846706 O        -1:word:颤\n",
      "4.842084 B-4      word:胃\n",
      "4.825317 B-4      word:肝\n",
      "4.802248 I-1      word:术\n",
      "4.791613 B-0      +2:word:０\n",
      "4.753887 I-4      +2:+1:0_word:下，已\n",
      "4.745822 O        word:8\n",
      "4.743717 O        word:6\n",
      "4.730944 I-3      +2:+1:0_word:音，结\n",
      "4.728918 O        word:行\n",
      "4.725357 B-3      +2:word:困\n",
      "4.692824 I-0      -1:word:肛\n",
      "4.670336 I-3      -1:0:发热\n",
      "4.665473 I-4      -1:0:外阴\n",
      "4.657390 B-3      +1:0:黄染\n",
      "4.641359 O        +2:+1:0_word:晕，无\n",
      "4.633129 O        +1:0:局部\n",
      "4.629233 O        -2:-1:0_word:可。左\n",
      "4.625200 I-4      -1:0:两肺\n",
      "\n",
      "Top negative:\n",
      "-7.381172 I-2      -1:word:病\n",
      "-6.129837 O        word:痰\n",
      "-5.846017 O        word:腹\n",
      "-5.779153 I-2      -1:word:炎\n",
      "-5.585396 B-1      bias\n",
      "-5.540635 I-4      -1:0:心肺\n",
      "-5.407365 O        word:鼻\n",
      "-5.231135 I-0      -1:word:音\n",
      "-5.200958 O        +1:0:无力\n",
      "-5.159814 I-2      -1:word:伤\n",
      "-5.111366 O        word:便\n",
      "-4.800465 I-0      -2:word:压\n",
      "-4.701160 I-1      -1:word:剂\n",
      "-4.535374 I-4      -2:word:间\n",
      "-4.407836 O        +2:+1:word:转移\n",
      "-4.333868 B-0      -1:word:性\n",
      "-4.332246 I-1      -1:word:液\n",
      "-4.286698 O        -1:word:渗\n",
      "-4.144527 O        word:钠\n",
      "-4.125207 B-4      -2:-1:0_word:报：双\n",
      "-4.124259 O        word:胸\n",
      "-3.999999 I-4      word:左\n",
      "-3.996825 I-4      -2:word:脐\n",
      "-3.992659 I-4      -1:word:腺\n",
      "-3.929289 O        word:颈\n",
      "-3.854777 O        word:额\n",
      "-3.851828 O        word:眼\n",
      "-3.850695 I-3      +2:word:进\n",
      "-3.833969 I-3      +2:+1:0_word:痛，腰\n",
      "-3.818238 I-4      -2:-1:0_word:，胸部\n",
      "-3.817076 I-4      -2:-1:word:下肢\n",
      "-3.764877 O        word:超\n",
      "-3.724674 O        word:素\n",
      "-3.684356 O        word:脐\n",
      "-3.681297 O        -1:word:、\n",
      "-3.659761 O        word:胃\n",
      "-3.638203 O        word:宫\n",
      "-3.624706 I-4      +2:word:均\n",
      "-3.620006 I-1      word:抗\n",
      "-3.608041 O        word:腰\n",
      "-3.569467 O        word:丸\n",
      "-3.555550 I-4      +1:0:部分\n",
      "-3.546656 O        word:头\n",
      "-3.502304 I-4      -1:word:部\n",
      "-3.488766 I-0      -2:word:功\n",
      "-3.429787 O        +2:word:洗\n",
      "-3.411664 O        +2:+1:0_word:及右膝\n",
      "-3.391400 I-0      +2:word:压\n",
      "-3.386066 O        word:背\n",
      "-3.383986 I-4      -1:word:管\n",
      "-3.355562 B-2      bias\n",
      "-3.352610 I-4      -2:-1:word:头皮\n",
      "-3.334363 O        word:P\n",
      "-3.327784 O        word:糖\n",
      "-3.317122 O        word:唇\n",
      "-3.309781 O        -2:word:消\n",
      "-3.295159 O        -1:word:核\n",
      "-3.289769 I-0      -1:word:R\n",
      "-3.232430 O        word:溃\n",
      "-3.217411 O        word:咽\n",
      "-3.216895 B-3      -2:-1:0_word:部无肿\n",
      "-3.210760 O        +2:word:素\n",
      "-3.207216 I-1      +1:word:甲\n",
      "-3.199726 I-2      +1:word:肠\n",
      "-3.176204 O        +1:word:、\n",
      "-3.121136 O        word:膝\n",
      "-3.105364 O        word:心\n",
      "-3.070750 O        word:牙\n",
      "-3.058274 O        word:耳\n",
      "-3.052402 O        +2:word:苍\n",
      "-3.049238 O        -1:word:和\n",
      "-3.048907 O        +2:word:养\n",
      "-3.031892 O        word:肾\n",
      "-3.030521 O        +2:+1:word:。自\n",
      "-3.020197 B-0      -2:-1:0_word:报：头\n",
      "-3.011325 I-4      -2:-1:word:肋骨\n",
      "-3.010881 O        word:腿\n",
      "-2.996681 O        word:白\n",
      "-2.991866 O        word:胰\n",
      "-2.986661 O        +1:word::\n",
      "-2.889854 O        -1:word:瘀\n",
      "-2.889042 I-4      word:痛\n",
      "-2.880749 I-4      -2:word:脊\n",
      "-2.861773 I-4      +2:word:.\n",
      "-2.861125 I-3      +2:+1:0_word:胀，无\n",
      "-2.857818 B-4      -2:-1:0_word:可，四\n",
      "-2.818930 B-4      -2:-1:word:多，\n",
      "-2.802610 I-0      +2:word:力\n",
      "-2.796863 I-4      -2:-1:word:、头\n",
      "-2.790916 B-4      -2:-1:0_word:部、腹\n",
      "-2.781216 O        -2:word:踝\n",
      "-2.781043 O        word:足\n",
      "-2.740984 B-0      -2:word:部\n",
      "-2.729350 O        +1:0:张。\n",
      "-2.713188 I-4      -2:-1:word:外侧\n",
      "-2.711256 I-4      -2:-1:0_word:：腰部\n",
      "-2.709157 O        -2:word:l\n",
      "-2.672358 O        word:臂\n",
      "-2.667024 B-4      -2:-1:0_word:差，二\n",
      "-2.656923 O        +2:+1:word:行阑\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(100))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-100:][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
